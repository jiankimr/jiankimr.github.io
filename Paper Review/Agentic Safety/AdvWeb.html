<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang xml:lang>
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>AdvWeb</title>
  <style>
html {
color: #1a1a1a;
background-color: #fdfdfd;
}
body {
margin: 0 auto;
max-width: 36em;
padding-left: 50px;
padding-right: 50px;
padding-top: 50px;
padding-bottom: 50px;
hyphens: auto;
overflow-wrap: break-word;
text-rendering: optimizeLegibility;
font-kerning: normal;
}
@media (max-width: 600px) {
body {
font-size: 0.9em;
padding: 12px;
}
h1 {
font-size: 1.8em;
}
}
@media print {
html {
background-color: white;
}
body {
background-color: transparent;
color: black;
font-size: 12pt;
}
p, h2, h3 {
orphans: 3;
widows: 3;
}
h2, h3, h4 {
page-break-after: avoid;
}
}
p {
margin: 1em 0;
}
a {
color: #1a1a1a;
}
a:visited {
color: #1a1a1a;
}
img {
max-width: 100%;
}
svg {
height: auto;
max-width: 100%;
}
h1, h2, h3, h4, h5, h6 {
margin-top: 1.4em;
}
h5, h6 {
font-size: 1em;
font-style: italic;
}
h6 {
font-weight: normal;
}
ol, ul {
padding-left: 1.7em;
margin-top: 1em;
}
li > ol, li > ul {
margin-top: 0;
}
blockquote {
margin: 1em 0 1em 1.7em;
padding-left: 1em;
border-left: 2px solid #e6e6e6;
color: #606060;
}
code {
font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
font-size: 85%;
margin: 0;
hyphens: manual;
}
pre {
margin: 1em 0;
overflow: auto;
}
pre code {
padding: 0;
overflow: visible;
overflow-wrap: normal;
}
.sourceCode {
background-color: transparent;
overflow: visible;
}
hr {
background-color: #1a1a1a;
border: none;
height: 1px;
margin: 1em 0;
}
table {
margin: 1em 0;
border-collapse: collapse;
width: 100%;
overflow-x: auto;
display: block;
font-variant-numeric: lining-nums tabular-nums;
}
table caption {
margin-bottom: 0.75em;
}
tbody {
margin-top: 0.5em;
border-top: 1px solid #1a1a1a;
border-bottom: 1px solid #1a1a1a;
}
th {
border-top: 1px solid #1a1a1a;
padding: 0.25em 0.5em 0.25em 0.5em;
}
td {
padding: 0.125em 0.5em 0.25em 0.5em;
}
header {
margin-bottom: 4em;
text-align: center;
}
#TOC li {
list-style: none;
}
#TOC ul {
padding-left: 1.3em;
}
#TOC > ul {
padding-left: 0;
}
#TOC a:not(:hover) {
text-decoration: none;
}
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}

ul.task-list[class]{list-style: none;}
ul.task-list li input[type="checkbox"] {
font-size: inherit;
width: 0.8em;
margin: 0 0.8em 0.2em -1.6em;
vertical-align: middle;
}
</style>
</head>
<body>
<header id="title-block-header">
<h1 class="title">AdvWeb</h1>
</header>
<p>요약</p>
<ul>
<li><p>VLM based web agent 기술이 발전하고 있으나, 악의적인 공격에 대한
안전성, 보안 문제가 충분히 탐구 되지 않은 상태이다. 배포하기에 심각한
우려가 제기된다.</p>
<p>→ web agent의 취약점을 밝히고 악용해봐야 한다.</p>
<p>→ web agent를 대상으로 하는 새로운 black-box 공격 프레임워크 advweb을
제안한다.</p></li>
</ul>
<p>Keywords</p>
<ul>
<li>gpt-4v based vlm web agent</li>
<li>black-box</li>
<li>adversarial prompter model training</li>
<li>Direct Policy Optimization (DPO)</li>
</ul>
<p>Contribution: adversarial string injection maintains <strong>stealth
and control</strong></p>
<ul>
<li>기존 접근법↔︎ 본 논문 adversarial string 삽입 방식: 스텔스성과 통제성
<ul>
<li>공격 전후의 웹사이트의 외관이 변하지 않아 사용자가 변조를 탐지하기
거의 불가능하고,</li>
<li>생성된 adversarial string 내 특정 부분 문자열을 변경하여 공격 목표를
쉽게 수정할 수 있다(예: 다른 회사의 주식을 구매하도록 설정). 이로써
공격의 유연성과 효율성이 크게 향상된다.</li>
</ul></li>
</ul>
<p>문제상황</p>
<ul>
<li>LLMs와 VLMs의 발전 → generalist web agents(현실 세계의 웹사이트와
상호작용하며 작업 수행)
<ul>
<li>web agents는 금융, 의료, 전자상거래와 같은 고위험 분야를 포함한
다양한 분야에서 인간의 생산성을 크게 향상할 잠재력을 지님</li>
<li>그러나 adversarial attacks에 대한 robustness 측면에서 보안 과제를
제시한다.</li>
</ul></li>
<li>최근 연구에서는, 현실 세계로 배포되기 전에 agent의 취약성을
드러내고자, adversarial attacks를 도입함.
<ul>
<li>기존 접근 방식은 수동으로 공격 프롬프트를 설계하는 데 인적 노력이
필요한 경우와 특정 공격 시나리오에 집중하는 경우에 비용이 많이 들고, 더
효율적이고 적응성 높은 공격 프레임워크 개발에는 한계가 있다.</li>
<li>LLM과 VLM에 대한 adversarial attack을 통해 공격 프롬프트를 자동으로
최적화하려는 시도가 있긴 하지만, 이러한 방법은 VLM 기반 에이전트를
대상으로 하는 유연성이 부족하며, 블랙박스 공격 설정에서는 효과적인
전이성을 달성하는 데 어려움을 겪는다.</li>
</ul></li>
</ul>
<p>관련 연구의 한계점</p>
<ul>
<li><p>Adversarial Attack on LLM</p>
<ul>
<li><p>token 기반 공격(token의 이산적 특성으로 인해 공격을 최적화하는
것이 이미지 기반 공격보다 어려움)</p>
<ul>
<li><p><strong>초기 연구</strong>(Ebrahimi et al., 2018; Wallace et al.,
2019; Shin et al., 2020)</p>
<p>특정 반응을 이끌어내거나 유해한 출력을 생성하기 위해, 토큰 수준에서
최적화된 공격 문자열을 사용함. 주로 greedy search이나 gradient
information을 활용하여 중요한 토큰을 수정하는 방식</p></li>
<li><p><strong>ARCA(</strong>Jones et al., 2023)</p>
<p>토큰 수준의 최적화를 정교화하여 여러 토큰을 동시에 교환함으로써
효율성을 높임</p></li>
<li><p><strong>GCG Attack</strong>(Zou et al., 2023)</p>
<p>공격의 효과를 향상하기 위해 긍정적인 responses을 유도하는 방식으로
공격 문자열을 최적화했음</p></li>
</ul>
<p>→ (한계점) token 기반 방법은 종종 가독성이 떨어지며, perplexity-based
detectors에 의해 쉽게 탐지된다.</p></li>
<li><p>다른 방법</p>
<ul>
<li><strong>AutoDan</strong>(Liu et al., 2024c) hierarchical genetic
algorithm을 사용하여 적대적 프롬프트의 스텔스성을 높임</li>
<li><strong>AmpleGCG</strong>(Liao &amp; Sun, 2024)와
<strong>AdvPrompter</strong>(Paulus et al., 2024) generative models을
직접 사용해 gradient based optimization에 의존하지 않고 adversarial
suffixes를 생성</li>
</ul>
<p>→(한계점) 유해 프롬프트에 대해 긍정적 반응을 이끌어낸다는 간단한
목표에 주로 초점, 특히 vlm based web agent에서 더 복잡한 공격 목표를
달성하기 어렵다.</p></li>
</ul>
<p>→ (개선안) 1. 복잡한 목표 다루기(ex. 구매 결정 조작) 2. 스텔스성,
통제성 유지</p></li>
<li><p>Web agent</p>
<ul>
<li><p>기존</p>
<ul>
<li><p>Nakano et al., 2021; Wu et al., 2024b</p>
<p>retrieval capabilities가 추가된 LLM을 사용해 generalist web agent를
구축한다.</p></li>
<li><p>최근 접근 방식(Yao et al., 2022; Zhou et al., 2023; Deng et al.,
2024</p>
<p>웹페이지의 <strong>HTML 입력</strong>을 직접 처리하여 인간의 지시에
기반해 시뮬레이션 혹은 현실적인 웹 환경에서 작업을 수행할 수 있는 웹
에이전트를 개발한다.</p></li>
</ul>
<p>→ (한계점) HTML 콘텐츠는 사람의 웹 탐색에 사용되는 시각적 정보에 비해
<strong>정보 밀도</strong>가 낮고 noise가 포함되어 있어 작업 성공률이
낮고 실용적인 배포가 어려운 문제가 있다.</p></li>
<li><p>개선</p>
<ul>
<li><p>SeeAct(Zheng et al., 2024) generalist web agent 프레임워크</p>
<p>웹페이지의 스크린샷을 입력으로 사용하는 두 단계의 파이프라인을 도입 →
추론 능력 및 작업 완수 성능 향상</p></li>
</ul></li>
</ul>
<p>→ Advweb은 SeeAct를 주요 공격 대상을 삼음.</p>
<p>→ 그러나, 웹 페이지 스크린샷 or html 콘텐츠를 입력으로 하는 다른 웹
에이전트에도 쉽게 적용가능</p></li>
<li><p>Existing Attacks against Web Agents</p>
<ul>
<li><p>Yang et al. (2024)과 Wang et al. (2024)</p>
<p>백본 모델을 backdoor attacks으로 미세 조정하여 웹 에이전트가 잘못된
결정을 내리도록 유도하는 방법을 탐구했다.</p></li>
<li><p>Wu et al., 2024b; Liao et al., 2024</p>
<p>웹 콘텐츠에 악의적인 지시를 삽입하여 웹 에이전트가 간접적인
프롬프트를 따르도록 유도해 부정확한 출력이나 민감한 정보를 유출하도록
하는 방법을 사용했다.</p>
<p>→ 악의적 지시는 수동으로 설계되고 heuristics에 의존하여 작성되기
때문에 확장성과 유연성에 한계가 있다.</p></li>
<li><p>Wu et al.(2024a) 웹 에이전트를 속이기 위한 auto 적대적 입력
최적화 방법을 도입</p>
<p><strong>→ 기울기 기반 최적화</strong>를 위한 화이트박스 접근이
필요하거나, 여러 <strong>CLIP 모델</strong>에 공격을 전이하는 데 한계를
보였다.</p></li>
</ul>
<p>→ AdvWeb은 black box <strong>설정</strong>에서 웹 에이전트를 공격하는
것을 목표로 한다.</p>
<p>RL 을 활용해 성공/실패 공격 시도의 피드백을 통합하여 학습→
웹에이전트를 공격하는 생성 모델을 훈련한다.</p></li>
</ul>
<p>제안</p>
<ul>
<li>generalist web agents 취약성 탐구 목적의 black box 제어 공격
프레임워크 AdvWeb</li>
<li>방식
<ul>
<li>웹 페이지에 보이지 않는 adversarial strings를 삽입하여, 에이전트가
특정 유해한 적대적 행동을 수행하도록 유도한다.
<ul>
<li>(ex) 잘못된 금융 거래나 부적절한 주식 구매</li>
</ul></li>
<li>피해 에이전트의 블랙박스 피드백을 기반으로 RL을 활용하여 adversarial
strings를 최적화하는 <strong>두 단계의 훈련 방식</strong>을 제안한다.
<ul>
<li><strong>Direct Policy Optimization (DPO)</strong> 기법을 적용하여
성공 및 실패한 공격 시도의 피드백을 학습에 활용함으로써, 공격의 유연성과
효율성을 높였다.</li>
<li>특히 AdvWeb은 특정 사용자 요청에 대해 다른 회사나 작업을 대상으로
하도록 적대적 문자열을 쉽게 조정할 수 있어 재최적화가 필요하지 않다.
<img role="img" aria-label="image.png" src="data:application/xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPEVycm9yPjxDb2RlPkFjY2Vzc0RlbmllZDwvQ29kZT48TWVzc2FnZT5BY2Nlc3MgRGVuaWVkPC9NZXNzYWdlPjxSZXF1ZXN0SWQ+UlI5N01BNUhSUUUzM0VRRDwvUmVxdWVzdElkPjxIb3N0SWQ+c3VBSUdaUjRyOEllS2xkeUdBcDdYV1pKM3I1T2Z3Z0ViNUZiVXc5WTB0K3RjVmszWlpvSWt0YnY3SjVSQ0M2WE85ZS9xMGJYRGRxeVhsL2dubmhNaXc9PTwvSG9zdElkPjwvRXJyb3I+" alt="image.png" /></li>
</ul></li>
</ul></li>
</ul>
<p>방법론</p>
<p>Targeted Black-box Attack against Web Agents</p>
<ul>
<li><p>Preliminaries on Web Agent Formulation</p>
<aside>
<p>💡</p>
<p>web agent (예: SeeAct) 의 설계 방식</p>
<ul>
<li><p>Input</p>
<ul>
<li>특정 웹사이트(예: 주식 거래 플랫폼)</li>
<li>작업 요청 <strong>T</strong>(예: “Microsoft 주식 한 주를
구매하라”)</li>
</ul></li>
<li><p>action</p>
<ul>
<li>웹사이트에서 작업 <strong>T</strong>를 성공적으로 완료하기 위해 실행
가능한 <strong>행동의 연속 {a1, a2, …, an}</strong>을 생성</li>
<li>action 결정법
<ul>
<li>각 시간 단계 t에서) 현재 환경의 관측치 <strong>st</strong></li>
<li>agent는 st 이전에 실행된 행동 <strong>At = {a1, a2, …, at-1}, 작업
T</strong>를 기반으로 행동 <strong>at</strong>을 결정한다.
</aside></li>
</ul></li>
</ul></li>
<li><p>seeact의 경우, 관측치 st는 2개의 구성요소</p>
<ol type="1">
<li><p>웹페이지의 <strong>HTML 콘텐츠 ht</strong></p></li>
<li><p>(1)에 대응하는 <strong>렌더링된 스크린샷 it =
I(ht)</strong></p></li>
</ol></li>
<li><p>VLM policy model(예: GPT-4V): Π</p></li>
<li><p>agent가 policy model을 통해 만드는 action: (관측치, 작업,
executed actions)</p>
<figure>
<img role="img" aria-label="image.png" src="data:application/xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPEVycm9yPjxDb2RlPkFjY2Vzc0RlbmllZDwvQ29kZT48TWVzc2FnZT5BY2Nlc3MgRGVuaWVkPC9NZXNzYWdlPjxSZXF1ZXN0SWQ+Q1E3WDdNNVBNMkE0QTdGMTwvUmVxdWVzdElkPjxIb3N0SWQ+L0wzaE1aM1RlYUFjRmxJUlBNek5zWE50NHVyV2FjRkJ0RVJ5WUc1TlFydlI1Q0lJQTN6U0dVNVM3M255aElPN3ZsdnJqck54YTU3bEtXK1ZHdnVLVUE9PTwvSG9zdElkPjwvRXJyb3I+" alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>agent는 st 이전에 실행된 행동 <strong>At = {a1, a2, …, at-1}, 작업
T</strong>를 기반으로 각 행동 <strong>at</strong>을 결정한다.</p></li>
</ul></li>
</ul>
<aside>
<p>💡</p>
<p>each action</p>
<ul>
<li><p><strong>각 행동 at은 (ot, rt, et)</strong>이라는 triplet으로
구성됨</p>
<ul>
<li>ot는 수행할 작업</li>
<li>rt는 작업에 대응하는 argument</li>
<li>et는 목표 HTML 요소</li>
</ul>
<p>예를 들어, <strong>거래 웹사이트</strong>에서 <strong>주식 이름을
입력</strong>하기 위해,</p>
<p>에이전트는 <strong>주식 이름인 Microsoft(rt)</strong>를 주식
<strong>입력 텍스트 상자(et)</strong>에 입력하는 동작(ot)을 수행할
것이다.</p></li>
<li><p>행동 at이 수행되면 웹사이트는 이에 따라 업데이트됨</p></li>
<li><p>에이전트는 작업이 완료될 때까지 이 과정을 반복함</p>
</aside></li>
</ul>
<p>(간결성을 위해 특별히 명시되지 않는 한 이후 식에서는 시각 t 생략)</p>
<ul>
<li><p>Threat Model</p>
<ul>
<li><p>Attack Objective</p>
<ul>
<li>에이전트의 행동을 targeted 공격으로 고려함
<ul>
<li><p>agent action(at = (ot,rt,et)) → targeted adversarial action:
<strong>a_adv = (o, r_adv, e)</strong>으로 변경</p></li>
<li><p>이때 에이전트는 동일한 작업을 수행하지만 잘못된 <strong>악의적
인수</strong>를 사용하게 된다.</p>
<p>예를 들어, <strong>Microsoft(r) 주식</strong>을 구매하라는 요청이
<strong>NVIDIA(r_adv) 주식</strong>을 구매하도록 공격받으면, 사용자는 큰
재정적 손실을 입을 수 있다.</p></li>
</ul></li>
</ul></li>
<li><p>Environment Access, Attack Scenarios</p>
<ul>
<li><p>black box 환경</p>
<ul>
<li>이유: 대부분 웹 에이전트가 vlm 기반이다. → 에이전트 프레임워크,
백엔드 모델 가중치, 또는 백엔드 모델의 로그에 접근할 수 없다.</li>
</ul></li>
<li><p>Attack Scenarios</p>
<aside>
<p>💡</p>
<ul>
<li><p>공격자 → 웹사이트 html 콘텐츠 h에 접근</p>
<pre><code>                  + html 콘텐츠 변경 가능(h → h_adv)                   </code></pre></li>
</ul>
</aside></li>
<li><p>위 설정은 현실적인 공격 시나리오임</p>
<p>예를 들어, 악의적인 웹사이트 개발자가 정기적인 유지 보수나 웹사이트
업데이트 과정에서 HTML 콘텐츠를 의도적으로 수정함으로써 이익을 얻을 수
있다.</p>
<p>또한, 악성 라이브러리를 사용한 웹페이지 개발로 인해 의도치 않게
공격이 발생할 수 있으며, 최근 CISA 보고서(Synopsys, 2024)에 따르면
이러한 라이브러리가 숨겨진 취약점을 초래할 수 있다.</p></li>
<li><p>Attack Constraints</p>
<ul>
<li><p>공격은 <strong>stealth</strong> 와 <strong>control</strong> 을
모두 만족해야 한다.</p></li>
<li><ol type="1">
<li>stealth 충족</li>
</ol>
<ul>
<li><p><strong>렌더링된 스크린샷 i = I(h)</strong>는 HTML 콘텐츠 h에
의해 영향을 받으므로 사용자가 공격을 탐지하지 못하게 렌더링된 이미지는
변경되지 않아야 한다.</p></li>
<li><p>즉, HTML 콘텐츠의 수정이 사용자에게 보이는 시각적 정보에 영향을
주지 않도록 한다.</p>
<aside>
<p>💡</p>
<ul>
<li><strong>I(h) = I(h_adv)</strong>
</aside></li>
</ul></li>
</ul></li>
<li><ol start="2" type="1">
<li>control 충족</li>
</ol>
<ul>
<li>공격자는 적대적 프롬프트<strong>만</strong> 수정하여
<strong>새로운</strong> 목표에 맞는 공격을 <strong>쉽게</strong> 수행할
수 있어야 하며, 에이전트와의 추가적인 상호작용이나 최적화는 필요하지
않아야 한다.</li>
</ul>
<aside>
<p>💡</p>
<p>목표 행동a_adv = (o, r_adv, e) → a’_adv = (o, r’_adv, e) 변경시,</p>
<p>공격자는 **D(h_adv, r_adv, r’_adv)**라는 결정론적 함수를 사용
가능</p>
<ul>
<li>기존의 적대적 HTML 콘텐츠 h_adv</li>
<li>원래의 목표 인수 r_adv</li>
<li>새로운 목표 인수 r’_adv</li>
<li>새로운 적대적 HTML 콘텐츠 h’_adv</li>
</ul>
<p>예를 들어, Microsoft(r) 주식을 구매해야 하는 요청이 NVIDIA(r_adv)
주식을 구매하도록 성공적으로 공격한 h_adv의 경우, h’_adv = D(h_adv,
“NVIDIA”, “Apple”)을 통해 목표를 Apple(r’_adv)로 변경하여 유연하게
공격을 제어할 수 있다.</p>
</aside></li>
</ul></li>
<li><p>Challenges of Attacks against Web Agents (총정리)</p>
<ol type="1">
<li><p><strong>Discrete space with stealthiness and controllability
constraints</strong></p>
<p>적대적 HTML 콘텐츠 h_adv의 결정 공간은 이산적이며 최적화가 본질적으로
어렵다. 또한, 공격은 스텔스성과 통제성이라는 두 가지 중요한 제약 조건을
충족해야 하며, 이는 최적화 경관을 복잡하게 만든다.</p></li>
<li><p><strong>Black-box attack</strong></p>
<p>블랙박스 환경에서는 모델의 매개변수나 기울기에 접근할 수 없으며,
사용할 수 있는 피드백은 에이전트가 적대적 입력에 반응한 결과뿐이다. 이
제약 조건은 기존의 최적화 기법을 비효과적으로 만들며, 기울기 정보를
사용하지 않고도 공격 공간을 효율적으로 탐색할 수 있는 보다 정교한 방법이
필요하다.</p></li>
<li><p><strong>Limited efficiency and scalability</strong></p>
<p>기존 접근법은 ****seed prompt를 설계하거나 공격 시나리오를 설계하는
데 많은 수작업이 필요하다. 이러한 수작업 과정은 특히 다양한 복잡한
작업을 대상으로 할 때 확장성과 유연성을 제한한다. 이러한 도전 과제를
극복하기 위해, <strong>RL</strong> 기반의 새로운 공격 프레임워크를
제안한다. 이 프레임워크는 이산 최적화 문제를 해결하는 동시에 스텔스성과
통제성을 보장하고, 블랙박스 공격 시나리오를 효과적으로 다룰 수 있도록
설계되었으며, 수작업의존성을 줄여 효율성을 높인다.</p></li>
</ol></li>
</ul></li>
</ul></li>
<li><p>AdvWeb: Controllable Black-box Attacks on Web Agents</p>
<ol type="1">
<li><p>스텔스성과 통제성 제약을 준수하도록 설계된 <strong>advarsarial
HTML 콘텐츠 h_adv의 검색 공간을 줄인다.</strong></p>
<p>→ 생성된 적대적 콘텐츠가 사용자에게 탐지되지 않도록 하고, 공격 목표를
다시 최적화할 필요 없이 유연하게 수정할 수 있도록 한다.</p></li>
<li><p><strong>웹 에이전트의 응답으로부터 얻은 긍정적 및 부정적 피드백을
활용한다. RLAIF을 사용하여 적대적 문자열 생성 과정을 효율적으로
최적화함</strong></p>
<p>→ 웹 에이전트를 대상으로 한 타겟형 블랙박스 공격에서 학습을 강화하고,
복잡한 블랙박스 웹 에이전트의 공격 패턴에 적응하도록 한다.</p></li>
<li><p><strong>자동</strong>으로 적대적 문자열을 생성하는 <strong>생성
모델</strong>을 사용</p>
<p>→ 수작업 의존성 최소화 → 공격 유연성, 확장성, 효율성 향상</p></li>
</ol></li>
<li><p>AdvWeb 주요 구성 요소</p>
<ol type="1">
<li><p>자동 공격 및 피드백 수집</p></li>
<li><p>AdvWeb에서의 적대적 프롬프터 모델 학습</p></li>
</ol></li>
<li><ol type="1">
<li>Automatic Attack and Feedback Collection</li>
</ol>
<ul>
<li><p><strong>Adversarial HTML Content Design</strong></p>
<aside>
<p>💡</p>
<p>benign HTML 콘텐츠 h</p>
<ul>
<li>프롬프트 q의 일부를 삽입</li>
</ul>
<p>= 적대적 버전 h_adv 제작</p>
</aside>
<p>적대적 HTML 콘텐츠 h_adv의 검색 공간:</p>
<p>고차원&amp;이산적으로 최적화 어려움, 스텔스성과 통제성 제약조건</p>
<p>→ h_adv의 검색 공간을 <strong>특정 방식</strong>으로 줄인다.</p>
<aside>
<p>💡</p>
<ul>
<li><p>삽입된 프롬프트 q는 웹사이트에서 렌더링되지 않도록 특정 HTML
필드나 속성(예: “aria-label” = q) 내에 숨겨진다.</p></li>
<li><p>또한, q를 다양한 목표 행동으로 전이 가능하게 만들기 위해
<strong>target argument</strong>에 대한 <strong>placeholder</strong>(예:
“{target argument}”)를 삽입하고,</p>
<p>모델이 먼저 placeholder가 포함된 프롬프트 템플릿을 생성한 후 목표에
맞는 인수를 채우도록 훈련한다.</p></li>
<li><p>검색 공간을 줄이기 위해 삽입 위치를 <strong>ground truth HTML
요소</strong>인 <strong>e</strong>에 고정한다.</p></li>
</ul>
<p>이러한 HTML 숨김 기법과 placeholder를 활용하여 검색 공간을 효과적으로
줄이고, 스텔스성 제약을 만족하며, 통제성을 위한 최적화 프로세스를
단순화한다.</p>
</aside></li>
<li><p><strong>Automatic Attack and Feedback Collection
Pipeline</strong></p>
<ul>
<li><p>초기 <strong>RL</strong> 훈련을 위해 긍정적, 부정적 레이블을 가진
다양한 학습 사례가 필요함</p></li>
<li><p>훈련 데이터의 다양성을 확보하기 위해,</p>
<ol type="1">
<li><strong>LLM</strong>을 공격 프롬프터로 활용하여 <strong>다양한 공격
프롬프트 세트 {q_i}</strong>를 생성한다.</li>
</ol>
<p>(algorithm2 방식)</p>
<ol start="2" type="1">
<li><p>생성된 적대적 프롬프트를 사용해 블랙박스 웹 에이전트를 대상으로
공격을 시도한다.</p></li>
<li><p>피드백을 바탕으로 positive signals와 negative signals로
구분한다.</p></li>
</ol>
<p>→ 이러한 분류된 피드백을 RL 훈련에 활용한다.</p></li>
</ul></li>
</ul></li>
<li><ol start="2" type="1">
<li>Training Adversarial Prompter Model in AdvWeb</li>
</ol>
<figure>
<img role="img" aria-label="image.png" src="data:application/xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPEVycm9yPjxDb2RlPkFjY2Vzc0RlbmllZDwvQ29kZT48TWVzc2FnZT5BY2Nlc3MgRGVuaWVkPC9NZXNzYWdlPjxSZXF1ZXN0SWQ+Q1E3SzFTRVJCUDVYUjNUNzwvUmVxdWVzdElkPjxIb3N0SWQ+alp4R0ZuZlZhRWRJS1BKMjlxdEtqVHcyVjE4bHByeVhsYStRZDhIU0pxbW0xOU01RUNvS2s4bTZtQ0I4K3IwR2NwM2dTWDBJa21WQ1EwN3RTSG1UbUE9PTwvSG9zdElkPjwvRXJyb3I+" alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<ul>
<li><p>다양한 웹 환경을 다루고 공격의 효율성과 확장성을 보장하기
위해,</p>
<p>프롬프터 모델을 훈련하여 적대적 jailbreaking prompt
<strong>q</strong>를 생성하고</p>
<p>이를 HTML 콘텐츠에 삽입하도록 한다.</p></li>
<li><p>다양한 적대적 프롬프트 간의 미묘한 차이를 더 잘 포착하기
위해,</p>
<p><strong>RLAIF</strong> 학습 방식 적용</p></li>
<li><p><strong>RLAIF</strong> 은 보통 훈련과정에서 불안정하기
때문에,</p>
<p>강화 학습 이전에 <strong>supervised fine-tuning (SFT)</strong> 단계를
추가하여 훈련을 안정화한다.</p>
<p>AdvWeb의 전체 훈련 프로세스</p>
<ol type="1">
<li><p>긍정적 적대적 프롬프트를 사용한 SFT</p></li>
<li><p>긍정적 및 부정적 프롬프트를 모두 사용하는 강화 학습.</p></li>
</ol>
<figure>
<img role="img" aria-label="image.png" src="data:application/xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPEVycm9yPjxDb2RlPkFjY2Vzc0RlbmllZDwvQ29kZT48TWVzc2FnZT5BY2Nlc3MgRGVuaWVkPC9NZXNzYWdlPjxSZXF1ZXN0SWQ+Q1hLUkVCTlhYNzJDOUpXNTwvUmVxdWVzdElkPjxIb3N0SWQ+UlBBRHIrWkRrQ202MmVKOWxwLzZQZHhONWxQMkJURnJKdzByZjUxS1ByZG9ERHI4aC90RXllUWVJMEtyZXpEUm5adm1oeVFiT2hRVU9ydjdUd1ZRbXc9PTwvSG9zdElkPjwvRXJyb3I+" alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<figure>
<img role="img" aria-label="image.png" src="data:application/xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPEVycm9yPjxDb2RlPkFjY2Vzc0RlbmllZDwvQ29kZT48TWVzc2FnZT5BY2Nlc3MgRGVuaWVkPC9NZXNzYWdlPjxSZXF1ZXN0SWQ+QVJFWFNOQldNVzc2NlJGUzwvUmVxdWVzdElkPjxIb3N0SWQ+a1JtNmwySTBMbUNlZVE3RmZ6RlhUMUdvblB6cFMyaURwbVlBZnpXTVRYU0pYaGJlMTB3bmd2NzlaQ0NvbzljVjhlNlNIYnovVGxQL3MzWWRiZjY4ZEE9PTwvSG9zdElkPjwvRXJyb3I+" alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<figure>
<img role="img" aria-label="image.png" src="data:application/xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPEVycm9yPjxDb2RlPkFjY2Vzc0RlbmllZDwvQ29kZT48TWVzc2FnZT5BY2Nlc3MgRGVuaWVkPC9NZXNzYWdlPjxSZXF1ZXN0SWQ+QVJFWEQ5WkgwSkJYM0tEUzwvUmVxdWVzdElkPjxIb3N0SWQ+RlQyS0tHVjdzNG5qeGNwQnQ1WDVlRy9zaTk4Q2xBbk9ybzU3MktreklxRmxMMkpWd1NWamc5REFWNjV0NXd1YXdhOEpYMHdlSjV6WGxqa1FGdll6cFE9PTwvSG9zdElkPjwvRXJyb3I+" alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure></li>
</ul></li>
<li><p><strong>(1) Supervised Fine-tuning in AdvWeb</strong></p></li>
</ul>
<p>SFT 단계에서는 프롬프터 모델 가중치 θ를 최적화하여 <strong>긍정적인
적대적 프롬프트의 가능성을 극대화</strong>한다.</p>
<figure>
<img role="img" aria-label="image.png" src="data:application/xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPEVycm9yPjxDb2RlPkFjY2Vzc0RlbmllZDwvQ29kZT48TWVzc2FnZT5BY2Nlc3MgRGVuaWVkPC9NZXNzYWdlPjxSZXF1ZXN0SWQ+Nk1UUE1IQjlKMjQyV1E4TTwvUmVxdWVzdElkPjxIb3N0SWQ+Q0JabzJuQldGYXFqcElqZDdtMDQyY2xNTG9jYlR1V3k1L3M5d2dOLzJqVzBBTm1KNzdQd3MreHNabW9Kdlhja0E2OGphdUU1dXIxdWRTak02MVM0dFE9PTwvSG9zdElkPjwvRXJyb3I+" alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>→ 모델이 긍정적 적대적 프롬프트의 분포를 학습</p>
<p>→ RL 단계 안전성 향상 + 모델은 web agent가 목표행동을 성공하도록
유도하는 프롬프트를 잘 생성</p>
<ul>
<li><p><strong>(2) Reinforcement Learning Using DPO</strong></p>
<ul>
<li>rl 목표: 공격 패턴의 미묘한 차이 포착&amp; 프롬프터를 공격 목적에 더
잘 맞추기 위함.</li>
<li>DPO (Direct Preference Optimization)를 사용하여 강화 학습 과정을
안정화함</li>
<li>프롬프터 모델의 가중치 θ 최적화</li>
</ul>
<p>σ 는 logistic<strong>,</strong> β는 기본 리퍼런스 폴리시인 πref로부터
편차를 조절하는 매개변수</p>
<p>πref는 이전 SFT 모델인 πSFT로 초기화되고 고정된 것</p>
<figure>
<img role="img" aria-label="image.png" src="data:application/xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPEVycm9yPjxDb2RlPkFjY2Vzc0RlbmllZDwvQ29kZT48TWVzc2FnZT5BY2Nlc3MgRGVuaWVkPC9NZXNzYWdlPjxSZXF1ZXN0SWQ+SFJUTjdLOFZRUEdNNjhORDwvUmVxdWVzdElkPjxIb3N0SWQ+L05nNTVBeS8vbkhJNHZaRWtZaU5PRGF2M1dwNHU0S1BrQTZsR1dXbWFxRTBPQ05nWllZZ1NmdkM2S2tyaUNTdERmWnFvSWR2Q3BNPTwvSG9zdElkPjwvRXJyb3I+" alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>이 최적화 프레임워크는 프롬프터 모델이 반복적으로 파라미터를 세밀하게
조정하여, 타겟 적대적 행동a_adv을 수행하도록 웹 에이전트를 오도할 수
있는 성공적인<strong>adversarial jailbreaking prompts</strong>를 생성할
가능성을 극대화할 수 있도록 한다.</p></li>
</ul>
<p>실험</p>
<ul>
<li><p><strong>Victim Web Agent</strong></p>
<ul>
<li><p>웹 에이전트 <strong>SeeAct</strong>(Zheng et al., 2024)</p>
<ul>
<li>다양한 독점 VLM(예: GPT-4V, Gemini 1.5)이 구동된다.</li>
</ul>
<aside>
<p>💡</p>
<p>동작 과정</p>
<ol type="1">
<li><p>작업과 웹페이지 스크린샷을 기반으로 행동 설명(“Microsoft”라는
주식 이름을 입력해야 한다”)생성</p></li>
<li><p>이를 해당 HTML 콘텐츠(ht)와 매핑하여 웹 환경과 상호작용(“주식
이름 입력” 텍스트 상자에 “Microsoft”를 입력해야 한다)</p></li>
</ol>
</aside></li>
</ul></li>
<li><p><strong>Dataset and Metrics</strong></p>
<ul>
<li><strong>Mind2Web</strong> 데이터셋(Deng et al., 2024)
<ul>
<li>LLM/VLM 기반 에이전트를 평가하기 위한 현실 세계의 웹사이트 데이터를
포함한다.</li>
<li>31개의 도메인에서 137개의 웹사이트와 2,350개의 작업이 포함되어
있다.</li>
<li>advweb에서는 <strong>잠재적으로 심각한 결과를 초래할 수 있는 중요한
이벤트가 포함된 작업에 초점을 맞추어</strong>, 4개의 도메인에서 440개의
작업을 선택하고 이를 학습용 240개와 테스트용 200개 작업으로 나눈다.</li>
</ul></li>
<li>주 평가 지표로는 <strong>Mind2Web</strong>의 평가 방식을 따름
<ul>
<li><strong>step-based</strong> attack success rate (ASR)을 사용하여
공격의 효과를 평가</li>
<li><strong>각 단계</strong>에서 에이전트가, 공격자가 목표로 한 적대적
행동 a_adv = (o, r_adv, e)를 정확히 생성하면 공격이 성공한 것으로
간주됨</li>
</ul></li>
</ul></li>
<li><p><strong>Implementation Details</strong></p>
<ul>
<li><p>LLM 기반 공격 프롬프터</p>
<p>→ 다양한 적대적 프롬프트를 생성하여 초기 학습 데이터</p>
<ul>
<li><strong>GPT-4</strong>를 백엔드로 활용하여 프롬프트를 생성하고,
10개의 프롬프트를 생성할 때 다양성을 높이기 위해 온도를 1로
설정한다.</li>
</ul></li>
<li><p>생성 적대적 프롬프터 모델 공격자</p>
<ul>
<li><strong>Mistral-7B-Instruct-v0.2</strong>(Jiang et al., 2023)에서
초기화하며,</li>
<li>첫 번째 SFT 단계에서는 학습률 1e-4, 배치 크기 32로 설정한다.</li>
<li>두 번째 DPO 단계에서는 학습률을 유지하지만, 배치 크기는 16으로
줄인다.</li>
</ul></li>
<li><p>SeeAct 백엔드로는 <strong>gpt-4-vision-preview</strong>와
<strong>gemini-1.5-flash</strong>를 사용한다.</p></li>
</ul></li>
<li><p><strong>Baselines</strong></p></li>
</ul>
<p>현재 AdvWeb과 동일한 환경에서 웹 에이전트에 대해 블랙박스 공격을
수행하는 기존 연구가 없기 때문에, 네 가지 최신 공격 기법을 설정에 맞게
변형하여 비교 기준으로 사용한다</p>
<ul>
<li><p><strong>GCG</strong> (Zou et al., 2023)</p>
<p>기울기 기반 최적화로 목표 모델에 대한 토큰 수준의 기울기를 활용하여
<strong>adversarial suffix string</strong>을 최적화하는 화이트박스
공격이다. 우리의 블랙박스 설정에서는 <strong>LLaVA-NeXT</strong>라는
강력한 오픈소스 VLM을 대상으로 공격을 최적화한 후, 공격을 에이전트로
전이하는 방식으로 활용한다.</p></li>
<li><p><strong>AutoDAN</strong> (Liu et al., 2024c)</p>
<p>유전자 알고리즘을 활용하여 목표 모델의
<strong>로그(logits)</strong>을 최적화하는 화이트박스 공격이다. 이 또한
LLaVA-NeXT에 대해 공격을 최적화한 후, 결과를 모델에 전이하는 방식으로
적용한다.</p></li>
<li><p><strong>COLD-Attack</strong> (Guo et al., 2024)</p>
<p>에너지 기반의 <strong>Langevin dynamics</strong>로 제약된 디코딩을
사용하는 알고리즘이다. 기울기 기반 접근을 요구하며, 대응하는 에너지
함수를 도입하여 유창하고 스텔스성이 높은 적대적 프롬프트를
생성한다.</p></li>
<li><p><strong>Catastrophic Jailbreak</strong> (Huang et al., 2024)</p>
<p>디코딩 방법의 변형을 통해 모델 정렬을 파괴하는 블랙박스 공격
알고리즘이다. 시스템 프롬프트를 제거하거나 디코딩 하이퍼 파라미터와
샘플링 방법을 변경하여 최소한의 계산 비용으로 모델 공격을
수행한다.</p></li>
</ul>
<p>실험 결과</p>
<ul>
<li><p>높은 성공률을 기록하며, 다양한 웹 도메인에서 GPT-4V 기반 SeeAct를
성공적으로 공격하는 데 있어 97.5%의 공격 성공률을 달성했다. 게다가
AdvWeb은 목표를 변경해도 추가 최적화 없이 98.5%의 성공률을 유지해 강력한
통제 가능성을 입증했다.</p></li>
<li><p>Effectiveness of AdvWeb</p>
<ul>
<li><p><strong>VLM-powered web agent 는 AdvWeb에 매우
취약함</strong></p>
<ul>
<li>GPT-4V 백엔드를 사용하는 SeeAct에서 평균 97.5% ASR</li>
<li>Gemini 1.5 백엔드를 사용하는 SeeAct에서는 99.8% ASR</li>
</ul></li>
<li><p>baselines보다 우수함</p>
<p>기존 기법들은 화이트박스 환경에서 기울기 기반 정보를 활용하여 목표
반응을 극대화하도록 설계되었지만, <strong>복잡한 타겟형 블랙박스 공격
시나리오에서는 ASR이 0%로 기록</strong>되었다. ↔︎ AdvWeb과 대비됨</p>
<figure>
<img role="img" aria-label="image.png" src="data:application/xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPEVycm9yPjxDb2RlPkFjY2Vzc0RlbmllZDwvQ29kZT48TWVzc2FnZT5BY2Nlc3MgRGVuaWVkPC9NZXNzYWdlPjxSZXF1ZXN0SWQ+SFJUVDFGSFNLOTdSRTVQRzwvUmVxdWVzdElkPjxIb3N0SWQ+M0FoaUp0Y2Z1cXhkWDNLL1RjQmoyRGdwWERMdDl0a0QzdmU3eTRXdXhJUjV6VDlabXlkZlEwOWV5aWF1RXkyMTZoWm1VbVZCTDBnPTwvSG9zdElkPjwvRXJyb3I+" alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure></li>
</ul></li>
<li><p>In-depth Analysis of AdvWeb</p>
<ul>
<li><p><strong>AdvWeb은 높은 통제성을 가지고 다양한 공격 목표를 설정
가능함</strong></p>
<ul>
<li>생성된 적대적 문자열을 수정하여 새로운 목표에 맞도록 변경 평균
<strong>98.5%의 ASR</strong> → 통제성 입증</li>
</ul></li>
<li><p><strong>AdvWeb은 유연하고 다양한 설정에 대한 전이성이
높음</strong></p>
<ul>
<li>성공적인 적대적 문자열을 다양한 설정으로 전이하여 평가했다. 예를
들어, 삽입 위치나 HTML 필드를 변경했을 때도 높은 성공률을 유지했으며,
위치 변화 시 <strong>71.0%</strong>, HTML 필드 변경 시 97.0%의 ASR을
기록했다.</li>
</ul></li>
<li><p><strong>모델 피드백의 차이점으로부터 학습하는 것이 공격 성능
향상에 기여함</strong></p>
<ul>
<li><p>SFT만 사용했을 때의 ASR과 SFT와 DPO를 결합한 경우를 비교한 결과,
블랙박스 모델의 피드백을 활용함으로써 공격 성공률이 크게 향상되었다.
특히, 평균 ASR이 69.5% (SFT 단독)에서 97.5% (DPO 적용 시)로
증가했다.</p>
<figure>
<img role="img" aria-label="image.png" src="data:application/xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPEVycm9yPjxDb2RlPkFjY2Vzc0RlbmllZDwvQ29kZT48TWVzc2FnZT5BY2Nlc3MgRGVuaWVkPC9NZXNzYWdlPjxSZXF1ZXN0SWQ+R1ZNRURXS0Q5RkVXRTBHMjwvUmVxdWVzdElkPjxIb3N0SWQ+bVZld3hqbnk4N2hTcDdlTU01TkgrKzBLa01tNlVRbXRQU3RHSVQ5TDRKZGpkNUY0ZjhrUHcwQSt5VEMyVVZSSzR0b0FxS3ROazd3PTwvSG9zdElkPjwvRXJyb3I+" alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure></li>
</ul></li>
<li><p><strong>전이 기반 블랙박스 공격은 타겟형 공격에서 ASR이
낮음</strong></p>
<ul>
<li>Gemini 1.5 백엔드를 사용하는 에이전트에 대해 GPT-4V를 대상으로
생성된 공격 문자열을 전이했을 때의 ASR이 낮았다. 이는 모델 피드백을
활용하는 블랙박스 공격이 전이 기반 알고리즘보다 성능이 우수함을
보여준다.</li>
</ul></li>
</ul></li>
</ul>
<case study>
<p>미세한 차이가 있는 적대적 프롬프트가 서로 다른 공격 결과를 초래함</p>
<figure>
<img role="img" aria-label="image.png" src="data:application/xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPEVycm9yPjxDb2RlPkFjY2Vzc0RlbmllZDwvQ29kZT48TWVzc2FnZT5BY2Nlc3MgRGVuaWVkPC9NZXNzYWdlPjxSZXF1ZXN0SWQ+NkhKNDJWUlE3U1owNzZSUjwvUmVxdWVzdElkPjxIb3N0SWQ+STRzTWdHSFAySDc5bCtHYU9vakZYUHZBeEVqT0ZIVWw5Sk1SSHZ4Q3Q3YVFMRCtZTGJqb1ZuUkZlNFFnZFcyN1RTVkZmY3NCbkE0PTwvSG9zdElkPjwvRXJyb3I+" alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<ol type="1">
<li>“you”를 “I”로 바꿨을 때 공격이 실패에서 성공으로 바뀌었다.</li>
<li>“previous”라는 단어를 추가하여 타겟 에이전트를 성공적으로 속일 수
있었다.</li>
</ol>
<p>실험 결과, 이와 같은 미세한 차이들이 ASR에 큰 영향을 미쳤다.
(수동으로 설계된 적대적 프롬프트 방식에서는 이러한 미세한 패턴 차이를
포착하기 어려우나, AdvWeb의 두 단계 학습 프로세스를 통해 프롬프트의
미묘한 변화를 효과적으로 학습할 수 있다.)</p>
<ol type="1">
<li>사용자가 Microsoft 주식을 추가하도록 에이전트에 지시하지만, AdvWeb이
생성한 적대적 삽입(q) 후에 에이전트는 NVIDIA 주식을 구매하게 된다.</li>
<li>사용자가 Tylenol의 부작용 정보를 요청하지만, 적대적 삽입 이후
에이전트는 Aspirin의 부작용을 검색하게 된다.</li>
</ol>
<p>(AdvWeb이 적대적 공격을 통해 웹 에이전트의 동작을 조작하는 데 있어
효과적임을 잘 보여준다.)</p>
<p>결론</p>
<ul>
<li>이번 연구의 제한사항으로는 공격 문자열 최적화가 <strong>오프라인
피드백에 의존한다는 점</strong>이 있다. 미래 연구에서는 블랙박스
에이전트로부터 실시간 피드백을 활용할 수 있는 적대적 프롬프터 모델을
개발하여, 공격을 실시간으로 최적화하는 방법을 탐구할 수 있다.</li>
<li>이번 연구는 웹 에이전트의 상대적으로 낮은 <strong>end-to-end task
completion rate</strong>(작업 완료율)로 인해 step-based ASR을 주 평가
지표로 삼았다. 단계 수준의 평가가 유용한 통찰을 제공하기는 하지만,
에이전트가 전체 사용자 요청을 완료하는 과정에서의 위험을 충분히
반영하지는 못할 수 있다.</li>
</ul>
<p>→ 실시간 인터랙티브 웹 환경 내에서 end-to-end 평가를 수행하는 방법을
고려해야 한다.</p>
<p>→ 작업 전체 흐름에서의 ASR을 모니터링하는 방법을 고려해야 할
것이다.</p>
<p>→ gpt-4-vision-preview와 gemini-1.5-flash기반 SeeAct에 현존하는
guardrail 기법을 추가</p>
</body>
</html>
